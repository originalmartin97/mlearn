{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "- [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is another ensemble technique for classification and regression. It can be viewed as a \"series circuit\" of base learners.\n",
    "\n",
    "\n",
    "- The idea of gradient boosting originates from [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) and [Jerome Friedman](https://en.wikipedia.org/wiki/Jerome_H._Friedman) (1999).\n",
    "\n",
    "\n",
    "- The diversity of the base learners is achieved by training them on different targets.\n",
    "\n",
    "\n",
    "- The base learners are regressors, both for classification and regression.\n",
    "\n",
    "\n",
    "- Usually, the base learners are decision tree regressors, but in theory they could be any regression algorithm.\n",
    "\n",
    "\n",
    "- Gradient Boosted Decision Trees (or Gradient Boosting Machine) is a \"swiss army knife\" method in machine learning. It is invariant to the scale of the feature values and performs well on a wide variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code of Training (w/o Learning Rate)\n",
    "<img src=\"../_img/gradient_boosting_algorithm.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "- instead of step size $\\gamma_m$, we use $\\eta \\cdot \\gamma_m$, where $\\eta \\in (0, 1]$\n",
    "- $\\eta<1$ implements the \"slow cooking\" idea, and in practice leads to better ensembles than $\\eta=1$\n",
    "\n",
    "<img src=\"../_img/slow_cooking.jpg\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Case: Gradient Boosting for Regression\n",
    "\n",
    "- the loss function is the squared loss: $L(y, F(x)) = \\frac{1}{2} \\left(y - F(x)\\right)^2$\n",
    "- the initial model is the average target: $F_0(x) = \\frac{1}{n} \\sum_{i=1}^n y_i$\n",
    "- pseudo-residuals: $r_{im} = y_i - F_{m-1}(x_i)$\n",
    "- $m$-th weak learner: train model $h_m$ on $\\{(x_i, r_{im})\\}_{i=1}^n$\n",
    "- weight of the $m$-th weak learner: $w_m = \\eta \\left[\\sum_{i=1}^n h_m(x_i)r_{im}\\right] / \\left[\\sum_{i=1}^n \\left(h_m(x_i)\\right)^2\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Implement a tree based gradient boosting regressor and evaluate it on the Boston Housing data set using 3-fold cross-validation! Use a maximal tree depth of 3! The metric should be RMSE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing data set.\n",
    "import pandas as pd\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "         'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('../_data/housing_data.txt', delim_whitespace=True, names=names)\n",
    "df = df.sample(len(df), random_state=42) # data shuffling\n",
    "X = df.values[:, :-1] # input matrix\n",
    "y = df['MEDV'].values # target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, eta=0.1, n_estimators=100, max_depth=3):\n",
    "        self.eta = eta\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.yhat0 = y.mean()\n",
    "        r = y - self.yhat0\n",
    " \n",
    "        self.trees = []\n",
    "        for k in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=42)\n",
    "            tree.fit(X, r)\n",
    "            h = tree.predict(X)\n",
    "            w = eta * (h @ r) / (h @ h)\n",
    "            r -= w * h\n",
    "            self.trees.append((w, tree))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = np.ones(len(X)) * self.yhat0\n",
    "        for w, tree in self.trees:\n",
    "            yhat += w * tree.predict(X)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.375684135715931"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate(re, X, y):\n",
    "    cv = KFold(3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for tr, te in cv.split(X):\n",
    "        re.fit(X[tr], y[tr])\n",
    "        yhat = re.predict(X)\n",
    "        rmse = mean_squared_error(y[te], yhat[te])**0.5\n",
    "        scores.append(rmse)\n",
    "    return np.mean(scores)\n",
    "\n",
    "evaluate(SimpleGradientBoostingRegressor(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Repeat the previous experiment using scikit-learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.36820868174328"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "evaluate(GradientBoostingRegressor(random_state=42), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Which tree depth gives the most accurate ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for max_depth in range(1, 13):\n",
    "    print(max_depth, end=' ')\n",
    "    rmse = evaluate(GradientBoostingRegressor(random_state=42, max_depth=max_depth), X, y)\n",
    "    res.append({\n",
    "        'max_depth': max_depth,\n",
    "        'rmse': rmse\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3/B**: How the training and test RMSE changes with the number of trees? (Use a simple train-test split for this experiment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Apply a random forest and a gradient boosting classifier on the Wisconsin Breast Cancer data set! Use stratified 10-fold cross-validation! The evaluation metric should be the ratio of correct classifications. For both ensemble methods, determine the maximal tree depth that gives the highest accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wisconsin Breast Cancer data set.\n",
    "import pandas as pd\n",
    "names = [\n",
    "    'Sample_code_number', 'Clump_Thickness', 'Uniformity_of_Cell_Size',\n",
    "    'Uniformity_of_Cell_Shape', 'Marginal_Adhesion', 'Single_Epithelial_Cell_Size',\n",
    "    'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class'\n",
    "]\n",
    "df = pd.read_csv('../_data/wisconsin_data.txt', sep=',', names=names, na_values='?')\n",
    "df = df.sample(len(df), random_state=42) # data shuffling\n",
    "df['Bare_Nuclei'].fillna(0, inplace=True)\n",
    "X = df[df.columns[1: -1]].values\n",
    "y = (df['Class'].values / 2 - 1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting on Steroids\n",
    "\n",
    "- [XGBoost](https://en.wikipedia.org/wiki/XGBoost) and [LightGBM](https://en.wikipedia.org/wiki/LightGBM) are a highly efficient and flexible implementations of gradient boosting.\n",
    "- XGBoost started as a research project by Tianqi Chen (in 2014).\n",
    "- LightGBM was introduced by Microsoft Research (in 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Compare XGBoost, LightGBM and scikit-learn's GradientBoostingClassifier on the Wisconsin Breast Cancer problem, in terms of speed and accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
